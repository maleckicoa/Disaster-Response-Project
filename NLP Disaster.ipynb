{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import libraries and load data from database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "#https://view6914b2f4-3001.udacity-student-workspaces.com/\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer , TfidfTransformer\n",
    "from sklearn.metrics import confusion_matrix, average_precision_score ,roc_auc_score, accuracy_score,classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# load data from database\n",
    "engine = create_engine('sqlite:///disaster_messages.db')\n",
    "df = pd.read_sql_table('disaster_messages', con=engine)\n",
    "df.drop(columns=['child_alone'], inplace=True) # drop child_alone because it only has 0 values\n",
    "df=df.head(4000)\n",
    "X = df['message']\n",
    "y = df.iloc[:,4:]\n",
    "y['related'].replace(2, 1, inplace=True)  # to fix for multiple classes needed for ROC_AUC parameter\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "#Classes are very unbalanced!\n",
    "#y.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather update - a cold front from Cuba that could pass over Haiti\n",
      "['weather', 'update', 'cold', 'front', 'cuba', 'could', 'pas', 'haiti'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    \n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", ' ', text.lower())\n",
    "    tokens = word_tokenize(text)  #tokenize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        if tok not in stopwords.words(\"english\"):                  # take out stop-words\n",
    "            clean_tok = lemmatizer.lemmatize(tok).lower().strip()  # lematize, normalize\n",
    "            clean_tokens.append(clean_tok)\n",
    "        \n",
    "        \n",
    "\n",
    "    return clean_tokens\n",
    "    pass\n",
    "\n",
    "for message in X[:1]:\n",
    "    tokens = tokenize(message)\n",
    "    print(message)\n",
    "    print(tokens, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline with RF\n",
    "\n",
    "pipeline_rf = Pipeline([\n",
    "    ('vect',CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidef', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(n_estimators=10))) #RandomForestClassifier()  SVC()\n",
    "])\n",
    "\n",
    "pipeline_rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred =pipeline_rf.predict(X_test)\n",
    "#y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(y_test.columns)):\n",
    "    cr=classification_report(y_test.iloc[:,i], y_pred[:,i])\n",
    "    #print(y_test.columns[i],cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model (f1 score, precision and recall )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_function(pred):\n",
    "    target_names = y.columns\n",
    "    pred=pred[:,1]\n",
    "\n",
    "\n",
    "    category=[]\n",
    "    accuracy=[]\n",
    "    pos_precis=[]\n",
    "    neg_precis=[]\n",
    "    pos_recall=[]\n",
    "    neg_recall=[]\n",
    "    f_one_pos=[]\n",
    "    f_one_neg=[]\n",
    "    roc_auc=[]\n",
    "\n",
    "\n",
    "    for i in range(0, len(y_test.columns)):\n",
    "        cr=classification_report(y_test.iloc[:,i],pred ) #avg / total\n",
    "\n",
    "        neg=cr.partition(' 0 ')[2]\n",
    "        pos=cr.partition(' 1 ')[2]\n",
    "\n",
    "        tn=target_names[i]   \n",
    "        category.append(tn)\n",
    "\n",
    "        accuracy.append(accuracy_score(y_test.iloc[:,i], pred))\n",
    "\n",
    "        pos_precis.append(pos.split()[0])\n",
    "        neg_precis.append(neg.split()[1])\n",
    "\n",
    "        pos_recall.append(pos.split()[1])\n",
    "        neg_recall.append(neg.split()[1])\n",
    "\n",
    "        f_one_pos.append(pos.split()[2])\n",
    "        f_one_neg.append(neg.split()[2])\n",
    "\n",
    "        try:\n",
    "            roc=roc_auc_score(y_test.iloc[:,i], y_pred[:,i])\n",
    "        except:\n",
    "            roc=0\n",
    "\n",
    "        roc_auc.append(roc)\n",
    "\n",
    "    performance = pd.DataFrame(list(zip(category, accuracy, pos_precis, neg_precis, pos_recall, neg_recall, f_one_pos, f_one_neg,roc_auc)), \\\n",
    "                               columns =['category','accuracy', 'precision_pos','precision_neg','recall_pos','recall_neg','f_one_pos','f_one_neg', 'roc_auc']) \n",
    "\n",
    "    cols = performance.columns.drop('category')\n",
    "    performance[cols] = performance[cols].apply(pd.to_numeric, errors='coerce')\n",
    "    performance['balanced_accuracy'] = (performance['recall_pos'] + performance['recall_neg'])/2\n",
    "    return performance.mean()\n",
    "\n",
    "\n",
    "\n",
    "print(test_function(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model (check other models and use grid search to find better parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f4a11b0612d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mcv_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_rf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#scoring = 'roc_auc' , verbose=2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mcv_rf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0my_pred_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_rf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "########################################## RF\n",
    "\n",
    "pipeline_rf = Pipeline([\n",
    "\n",
    "('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "('tfidf', TfidfTransformer()),\n",
    "('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "parameters = {\n",
    "#'vect__max_features': (None, 5000), \n",
    "'clf__estimator__n_estimators': [10, 30],\n",
    "'clf__estimator__min_samples_split': [2, 3, 4],\n",
    "'clf__estimator__criterion': ['entropy', 'gini']\n",
    "#'tfidf__use_idf': (True, False),\n",
    "#'vect__max_n': (1, 2),\n",
    "#'vect__max_df': (0.5, 0.75, 1.0),\n",
    "#'vect__max_features': (None, 5000, 10000, 50000)}\n",
    "}\n",
    "\n",
    "cv_rf = GridSearchCV(pipeline_rf, param_grid=parameters,n_jobs=4) #scoring = 'roc_auc' , verbose=2\n",
    "cv_rf.fit(X_train, y_train)\n",
    "y_pred_rf = cv_rf.predict(X_test)\n",
    "\n",
    "print(\"\\nBest Parameters:\", cv_rf.best_params_)\n",
    "\n",
    "print(test_function(y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Parameters: {'clf__estimator__penalty': 'l1', 'vect__max_df': 0.5, 'vect__max_features': None}\n",
      "accuracy             0.553661\n",
      "precision_pos        0.154286\n",
      "precision_neg        0.554286\n",
      "recall_pos           0.648571\n",
      "recall_neg           0.554286\n",
      "f_one_pos            0.185143\n",
      "f_one_neg            0.681429\n",
      "roc_auc              0.000000\n",
      "balanced_accuracy    0.601429\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#################################LR\n",
    "pipeline_lr = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(LogisticRegression()))\n",
    "     ])\n",
    "\n",
    "parameters = {\n",
    "\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    #'vect__max_n': (1, 2),\n",
    "    'vect__max_features': (None, 5000),\n",
    "    \"clf__estimator__penalty\":(\"l1\",\"l2\")\n",
    "}\n",
    "\n",
    "cv_lr = GridSearchCV(pipeline_lr, param_grid=parameters, n_jobs=-1) #, verbose=2\n",
    "\n",
    "cv_lr.fit(X_train, y_train)\n",
    "y_pred_lr = cv_lr.predict(X_test)\n",
    "\n",
    "print(\"\\nBest Parameters:\", cv_lr.best_params_)\n",
    "\n",
    "print(test_function(y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Parameters: {'clf__estimator__C': 1, 'clf__estimator__kernel': 'linear', 'vect__max_features': None}\n",
      "accuracy             0.556696\n",
      "precision_pos        0.155429\n",
      "precision_neg        0.557143\n",
      "recall_pos           0.726286\n",
      "recall_neg           0.557143\n",
      "f_one_pos            0.187143\n",
      "f_one_neg            0.684000\n",
      "roc_auc              0.578902\n",
      "balanced_accuracy    0.641714\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "################################## SVC\n",
    "pipeline_svc = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(SVC()))\n",
    "     ])\n",
    "\n",
    "parameters = {\n",
    "    'clf__estimator__kernel': ['linear', 'rbf', 'poly'],\n",
    "    'clf__estimator__C':[1, 10, 100],\n",
    "    'vect__max_features': (None, 5000)\n",
    "}\n",
    "\n",
    "cv_svc = GridSearchCV(pipeline_svc, param_grid=parameters, n_jobs=-1) #, verbose=2\n",
    "\n",
    "cv_svc.fit(X_train, y_train)\n",
    "y_pred_svc = cv_svc.predict(X_test)\n",
    "\n",
    "print(\"\\nBest Parameters:\", cv_svc.best_params_)\n",
    "\n",
    "print(test_function(y_pred_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "pipeline_xgb = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(GradientBoostingClassifier()))\n",
    "     ])\n",
    "\n",
    "parameters = {\n",
    "    'clf__estimator__max_depth': [3, 5, 10],\n",
    "    'clf__estimator__n_estimators':[100, 300,500],\n",
    "}\n",
    "\n",
    "cv_xgb = GridSearchCV(pipeline_xgb, param_grid=parameters, n_jobs=-1) #, verbose=2\n",
    "\n",
    "cv_xgb.fit(X_train, y_train)\n",
    "y_pred_xgb = cv_xgb.predict(X_test)\n",
    "\n",
    "print(\"\\nBest Parameters:\", cv_xgb.best_params_)\n",
    "\n",
    "print(test_function(y_pred_xgb))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
